{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test FoldAgent React Agent Loop with Local Search\n",
    "\n",
    "This notebook tests your custom React agent loop from `verl/experimental/agent_loop/FoldAgent` by:\n",
    "- Launching a stub LocalSearch FastAPI server (`/search`, `/open`).\n",
    "- Registering FoldAgent's `react_agent` and composing VERL config via Hydra.\n",
    "- Starting a standalone rollout server and driving one sample through the agent loop.\n",
    "- Inspecting the generated messages, reward, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_worker/__main__.py\", line 10, in <module>\n",
      "    from torch._inductor.async_compile import pre_fork_setup\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2611, in <module>\n",
      "    from torch import _meta_registrations\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\", line 12, in <module>\n",
      "    from torch._decomp import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\", line 276, in <module>\n",
      "    import torch._decomp.decompositions\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_decomp/decompositions.py\", line 16, in <module>\n",
      "    import torch._prims as prims\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_prims/__init__.py\", line 525, in <module>\n",
      "    abs = _make_elementwise_unary_prim(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_prims/__init__.py\", line 493, in _make_elementwise_unary_prim\n",
      "    return _make_prim(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_prims/__init__.py\", line 321, in _make_prim\n",
      "    prim_def = torch.library.custom_op(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\", line 173, in custom_op\n",
      "    return inner(fn)\n",
      "           ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\", line 154, in inner\n",
      "    result = CustomOpDef(namespace, opname, schema_str, fn)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\", line 204, in __init__\n",
      "    self._register_to_dispatcher()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\", line 624, in _register_to_dispatcher\n",
      "    lib._register_fake(self._name, fake_impl, _stacklevel=4)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/library.py\", line 193, in _register_fake\n",
      "    source = torch._library.utils.get_source(_stacklevel + 1)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_library/utils.py\", line 45, in get_source\n",
      "    def get_source(stacklevel: int) -> str:\n",
      "    \n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataProto\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent_loop\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent_loop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentLoopManager\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent_loop\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mFoldAgent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReactAgentLoop  \u001b[38;5;66;03m# Ensures @register(\"react_agent\") runs\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrollout\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreplica\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_rollout_replica_class\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tiger/verl_context_folding/verl/experimental/agent_loop/__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2024 Bytedance Ltd. and/or its affiliates\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent_loop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentLoopBase, AgentLoopManager, AgentLoopWorker, AsyncLLMServerManager\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msingle_turn_agent_loop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SingleTurnAgentLoop\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtool_agent_loop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToolAgentLoop\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tiger/verl_context_folding/verl/experimental/agent_loop/agent_loop.py:35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent_loop\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprometheus_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m update_prometheus_config\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent_loop\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resolve_config_path\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreward\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RewardLoopWorker\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataProto\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msingle_controller\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RayResourcePool, RayWorkerGroup\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tiger/verl_context_folding/verl/experimental/reward/__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2024 Bytedance Ltd. and/or its affiliates\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreward_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RewardLoopManager, RewardLoopWorker\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreward_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RewardModelManager\n\u001b[32m     18\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mRewardModelManager\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRewardLoopWorker\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRewardLoopManager\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tiger/verl_context_folding/verl/experimental/reward/reward_manager.py:34\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m copy_to_local\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreward_loop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_reward_loop_manager_cls\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreward_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RewardModelManager\n\u001b[32m     36\u001b[39m logger = logging.getLogger(\u001b[34m__file__\u001b[39m)\n\u001b[32m     37\u001b[39m logger.setLevel(os.getenv(\u001b[33m\"\u001b[39m\u001b[33mVERL_LOGGING_LEVEL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWARN\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tiger/verl_context_folding/verl/experimental/reward/reward_model.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msingle_controller\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RayResourcePool, split_resource_pool\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HFModelConfig, RewardModelConfig\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrollout\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreplica\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_rollout_replica_class\n\u001b[32m     23\u001b[39m logger = logging.getLogger(\u001b[34m__file__\u001b[39m)\n\u001b[32m     24\u001b[39m logger.setLevel(os.getenv(\u001b[33m\"\u001b[39m\u001b[33mVERL_LOGGING_LEVEL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWARN\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tiger/verl_context_folding/verl/workers/rollout/replica.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ActorHandle\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msingle_controller\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RayClassWithInitArgs, RayWorkerGroup\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mppo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mray_trainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RayResourcePool, ResourcePoolManager\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m omega_conf_to_dataclass\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HFModelConfig, RolloutConfig\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tiger/verl_context_folding/verl/trainer/ppo/ray_trainer.py:54\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mppo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreward\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_reward, compute_reward_async\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mppo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Role, WorkerType, need_critic, need_reference_policy, need_reward_model\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_latest_ckpt_path, should_save_ckpt_esi\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m omega_conf_to_dataclass\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdebug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m marked_timer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tiger/verl_context_folding/verl/utils/checkpoint/__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2024 Bytedance Ltd. and/or its affiliates\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint_handler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CheckpointHandler, OrchestrationMode\n\u001b[32m     17\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mCheckpointHandler\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mOrchestrationMode\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tiger/verl_context_folding/verl/utils/checkpoint/checkpoint_handler.py:29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_latest_ckpt_path, get_checkpoint_tracker_filename\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m log_with_rank\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEngine\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_step\u001b[39m(path):\n\u001b[32m     33\u001b[39m     match = re.search(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mglobal_step_(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+)\u001b[39m\u001b[33m\"\u001b[39m, path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tiger/verl_context_folding/verl/workers/engine/__init__.py:28\u001b[39m\n\u001b[32m     25\u001b[39m     MindspeedEngineWithLMHead = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmegatron\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MegatronEngine, MegatronEngineWithLMHead\n\u001b[32m     30\u001b[39m     __all__ += [\u001b[33m\"\u001b[39m\u001b[33mMegatronEngine\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMegatronEngineWithLMHead\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tiger/verl_context_folding/verl/workers/engine/megatron/__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2024 Bytedance Ltd. and/or its affiliates\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformer_impl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MegatronEngine, MegatronEngineWithLMHead\n\u001b[32m     17\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mMegatronEngine\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMegatronEngineWithLMHead\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tiger/verl_context_folding/verl/workers/engine/megatron/transformer_impl.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parallel_state \u001b[38;5;28;01mas\u001b[39;00m mpu\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline_parallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_forward_backward_func\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01momegaconf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OmegaConf\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Megatron-LM/megatron/core/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor_parallel\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parallel_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Megatron-LM/megatron/core/tensor_parallel/__init__.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_entropy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vocab_parallel_cross_entropy\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m broadcast_data\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     ColumnParallelLinear,\n\u001b[32m      6\u001b[39m     RowParallelLinear,\n\u001b[32m      7\u001b[39m     VocabParallelEmbedding,\n\u001b[32m      8\u001b[39m     copy_tensor_model_parallel_attributes,\n\u001b[32m      9\u001b[39m     linear_with_grad_accumulation_and_async_allreduce,\n\u001b[32m     10\u001b[39m     param_is_not_tensor_parallel_duplicate,\n\u001b[32m     11\u001b[39m     set_defaults_if_not_set_tensor_model_parallel_attributes,\n\u001b[32m     12\u001b[39m     set_tensor_model_parallel_attributes,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmappings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     all_gather_last_dim_from_tensor_parallel_region,\n\u001b[32m     16\u001b[39m     all_to_all,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     scatter_to_tensor_model_parallel_region,\n\u001b[32m     27\u001b[39m )\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrandom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     CheckpointWithoutOutput,\n\u001b[32m     30\u001b[39m     checkpoint,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     model_parallel_cuda_manual_seed,\n\u001b[32m     35\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Megatron-LM/megatron/core/tensor_parallel/layers.py:32\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     divide,\n\u001b[32m     23\u001b[39m     get_pg_rank,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     prepare_input_tensors_for_wgrad_compute,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdist_checkpointing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmapping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardedStateDict\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_sharded_tensors_for_checkpoint\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmappings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     34\u001b[39m     copy_to_tensor_model_parallel_region,\n\u001b[32m     35\u001b[39m     gather_from_sequence_parallel_region,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     scatter_to_tensor_model_parallel_region,\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrandom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_cuda_rng_tracker, get_expert_parallel_rng_tracker_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Megatron-LM/megatron/core/transformer/__init__.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspec_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModuleSpec, build_module\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformer_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MLATransformerConfig, TransformerConfig\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformer_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransformerLayer, TransformerLayerSubmodules\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Megatron-LM/megatron/core/transformer/transformer_layer.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpacked_seq_params\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PackedSeqParams\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocess_groups_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelCommProcessGroups\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcuda_graphs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CudaGraphManager\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menums\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayerType\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity_op\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IdentityFuncOp, IdentityOp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Megatron-LM/megatron/core/transformer/cuda_graphs.py:15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tree_flatten\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parallel_state\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor_parallel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrandom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CudaRNGStatesTracker,\n\u001b[32m     17\u001b[39m     get_all_rng_states,\n\u001b[32m     18\u001b[39m     get_cuda_rng_tracker,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity_op\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IdentityOp\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmegatron\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MegatronModule\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Megatron-LM/megatron/core/tensor_parallel/random.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gather_split_1d_tensor, split_tensor_into_1d_equal_chunks\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformer_engine\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformer_engine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activation_recompute_forward\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformer_engine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfp8\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FP8GlobalStateManager, fp8_autocast\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformer_engine/__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformer_engine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pytorch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformer_engine/common/__init__.py:128\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNVTE_PROJECT_BUILDING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mint\u001b[39m(os.getenv(\u001b[33m\"\u001b[39m\u001b[33mNVTE_RELEASE_BUILD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m))):\n\u001b[32m    127\u001b[39m     _CUDNN_LIB_CTYPES = _load_cudnn()\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     _NVRTC_LIB_CTYPES = \u001b[43m_load_nvrtc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m     _TE_LIB_CTYPES = _load_library()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformer_engine/common/__init__.py:104\u001b[39m, in \u001b[36m_load_nvrtc\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Attempt to locate NVRTC in CUDA_HOME, CUDA_PATH or /usr/local/cuda\u001b[39;00m\n\u001b[32m    103\u001b[39m cuda_home = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mCUDA_HOME\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mCUDA_PATH\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m/usr/local/cuda\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m libs = \u001b[43mglob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcuda_home\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/**/libnvrtc.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43m_get_sys_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m libs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mstub\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlibnvrtc-builtins\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x), libs))\n\u001b[32m    106\u001b[39m libs.sort(reverse=\u001b[38;5;28;01mTrue\u001b[39;00m, key=os.path.basename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/glob.py:28\u001b[39m, in \u001b[36mglob\u001b[39m\u001b[34m(pathname, root_dir, dir_fd, recursive, include_hidden)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mglob\u001b[39m(pathname, *, root_dir=\u001b[38;5;28;01mNone\u001b[39;00m, dir_fd=\u001b[38;5;28;01mNone\u001b[39;00m, recursive=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     14\u001b[39m         include_hidden=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a list of paths matching a pathname pattern.\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33;03m    The pattern may contain simple shell-style wildcards a la\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m \u001b[33;03m    zero or more directories and subdirectories.\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(iglob(pathname, root_dir=root_dir, dir_fd=dir_fd, recursive=recursive,\n\u001b[32m     29\u001b[39m                       include_hidden=include_hidden))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/glob.py:97\u001b[39m, in \u001b[36m_iglob\u001b[39m\u001b[34m(pathname, root_dir, dir_fd, recursive, dironly, include_hidden)\u001b[39m\n\u001b[32m     95\u001b[39m     glob_in_dir = _glob0\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dirname \u001b[38;5;129;01min\u001b[39;00m dirs:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mglob_in_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m                           \u001b[49m\u001b[43minclude_hidden\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_hidden\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     99\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m os.path.join(dirname, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/glob.py:106\u001b[39m, in \u001b[36m_glob1\u001b[39m\u001b[34m(dirname, pattern, dir_fd, dironly, include_hidden)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_glob1\u001b[39m(dirname, pattern, dir_fd, dironly, include_hidden=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     names = \u001b[43m_listdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m include_hidden \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(pattern):\n\u001b[32m    108\u001b[39m         names = (x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m names \u001b[38;5;28;01mif\u001b[39;00m include_hidden \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/glob.py:177\u001b[39m, in \u001b[36m_listdir\u001b[39m\u001b[34m(dirname, dir_fd, dironly)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_listdir\u001b[39m(dirname, dir_fd, dironly):\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib.closing(_iterdir(dirname, dir_fd, dironly)) \u001b[38;5;28;01mas\u001b[39;00m it:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(it)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/glob.py:159\u001b[39m, in \u001b[36m_iterdir\u001b[39m\u001b[34m(dirname, dir_fd, dironly)\u001b[39m\n\u001b[32m    157\u001b[39m     arg = os.curdir\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m os.scandir(arg) \u001b[38;5;28;01mas\u001b[39;00m it:\n\u001b[32m    160\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[32m    161\u001b[39m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import socket\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "import fastapi\n",
    "import uvicorn\n",
    "from starlette.requests import Request\n",
    "from starlette.responses import JSONResponse\n",
    "\n",
    "from hydra import compose, initialize_config_dir\n",
    "\n",
    "import verl\n",
    "from verl import DataProto\n",
    "from verl.experimental.agent_loop.agent_loop import AgentLoopManager\n",
    "from verl.experimental.agent_loop.FoldAgent import ReactAgentLoop  # Ensures @register(\"react_agent\") runs\n",
    "\n",
    "from verl.workers.rollout.replica import get_rollout_replica_class\n",
    "\n",
    "# Speed-focused Ray init; adjust as needed\n",
    "ray.init(runtime_env={\"env_vars\": {\"VLLM_USE_V1\": \"1\"}}, ignore_reinit_error=True)\n",
    "verl_config_dir = os.path.join(os.path.dirname(verl.__file__), \"trainer/config\")\n",
    "\n",
    "rollout_name = \"vllm\"  # or \"sglang\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Launch a stub LocalSearch FastAPI service\n",
    "Implements `/search` and `/open` endpoints expected by `LocalSearch` env. We set `LOCAL_SEARCH_URL` to point the agent to this server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL_SEARCH_URL: http://10.122.252.202:60219\n"
     ]
    }
   ],
   "source": [
    "@ray.remote(num_cpus=1)\n",
    "class SearchServer:\n",
    "    \"\"\"Minimal LocalSearch server with /search and /open endpoints.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.address = ray._private.services.get_node_ip_address()\n",
    "        self.port = self._get_free_port()\n",
    "        # Simple corpus\n",
    "        self.pages = {\n",
    "            \"wiki:elon\": {\n",
    "                \"docid\": \"wiki:elon\",\n",
    "                \"url\": \"https://en.wikipedia.org/wiki/Elon_Musk\",\n",
    "                \"text\": \"Elon Musk is the CEO of Tesla, Inc. He also leads SpaceX, xAI, and other ventures.\"\n",
    "            },\n",
    "            \"tesla:leadership\": {\n",
    "                \"docid\": \"tesla:leadership\",\n",
    "                \"url\": \"https://www.tesla.com/leadership\",\n",
    "                \"text\": \"Tesla's CEO is Elon Musk. The leadership page lists executive roles and bios.\"\n",
    "            },\n",
    "            \"news:tesla\": {\n",
    "                \"docid\": \"news:tesla\",\n",
    "                \"url\": \"https://example.com/news/tesla\",\n",
    "                \"text\": \"Breaking: Tesla maintains its leadership under CEO Elon Musk, focusing on EV innovation.\"\n",
    "            }\n",
    "        }\n",
    "        asyncio.create_task(self._start_fastapi_server())\n",
    "\n",
    "    def _get_free_port(self):\n",
    "        with socket.socket() as sock:\n",
    "            sock.bind((\"\", 0))\n",
    "            return sock.getsockname()[1]\n",
    "\n",
    "    async def _start_fastapi_server(self):\n",
    "        app = fastapi.FastAPI()\n",
    "\n",
    "        @app.post(\"/search\")\n",
    "        async def search(request: Request):\n",
    "            req = await request.json()\n",
    "            query = (req.get(\"query\") or \"\").lower()\n",
    "            k = int(req.get(\"k\", 10))\n",
    "            # naive keyword filter\n",
    "            def match(p):\n",
    "                txt = (p.get(\"text\") or \"\").lower()\n",
    "                return (\"elon\" in query or \"tesla\" in query) and (\"elon\" in txt or \"tesla\" in txt)\n",
    "            results = [p for p in self.pages.values() if match(p)][:k]\n",
    "            return JSONResponse(content={\"results\": results})\n",
    "\n",
    "        @app.post(\"/open\")\n",
    "        async def open_page(request: Request):\n",
    "            req = await request.json()\n",
    "            docid = req.get(\"docid\")\n",
    "            url = req.get(\"url\")\n",
    "            page = None\n",
    "            if docid and docid in self.pages:\n",
    "                page = self.pages[docid]\n",
    "            elif url:\n",
    "                for p in self.pages.values():\n",
    "                    if p.get(\"url\") == url:\n",
    "                        page = p\n",
    "                        break\n",
    "            return JSONResponse(content={\"results\": [page] if page else []})\n",
    "\n",
    "        config = uvicorn.Config(app, host=[\"::\", \"0.0.0.0\"], port=self.port, log_level=\"warning\")\n",
    "        server = uvicorn.Server(config)\n",
    "        await server.serve()\n",
    "\n",
    "    async def get_server_address(self) -> str:\n",
    "        return f\"{self.address}:{self.port}\"\n",
    "\n",
    "search_server = SearchServer.remote()\n",
    "search_address = ray.get(search_server.get_server_address.remote())\n",
    "os.environ[\"LOCAL_SEARCH_URL\"] = f\"http://{search_address}\"  # used by LocalSearch env\n",
    "print(\"LOCAL_SEARCH_URL:\", os.environ[\"LOCAL_SEARCH_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Compose VERL config and start standalone rollout\n",
    "We set the rollout engine (vLLM or SGLang), model path, and select our agent loop name `react_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130a815915324bed90f9da41145597fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1189056/4023949681.py:7: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize_config_dir(config_dir=verl_config_dir):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plugin config: {'workflow': 'search', 'max_turn': 32, 'retry_cjk': 0, 'turn_max_new_tokens': 1024, 'max_session': 3, 'val_max_session': 3, 'session_timeout': 3600, 'enable_summary': True, 'branch_len': 256, 'process_reward': 'flat,scope', 'max_traj': 2, 'must_finish': False, 'double_check': False, 'must_search': True, 'val_max_turn': 32, 'val_response_length': 1024}\n",
      "INFO 12-13 07:19:42 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tiger/verl_context_folding/verl/utils/profiler/config.py:49: UserWarning: Torch profiler tool config is not fully supported now.\n",
      "  warnings.warn(\"Torch profiler tool config is not fully supported now.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=1201697, ip=10.122.252.202) INFO 12-13 07:19:54 [__init__.py:235] Automatically detected platform cuda.\n",
      "(pid=1201918, ip=10.122.252.202) INFO 12-13 07:20:04 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) WARNING:2025-12-13 07:20:10,395:rollout mode is RolloutMode.STANDALONE, load_format is dummy, set to auto\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO:2025-12-13 07:20:10,395:vLLMHttpServer, replica_rank: 0, master address: 10.122.252.202, master port: 38159, data parallel master port: 47337\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO:2025-12-13 07:20:10,401:override_generation_config: {'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'repetition_penalty': 1.0, 'max_new_tokens': 4096}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) ['serve',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '/home/tiger/Qwen/Qwen3-1.7B',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--dtype',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  'bfloat16',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--load_format',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  'auto',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--max_model_len',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '8192',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--max_num_seqs',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '1024',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--enable_chunked_prefill',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--max_num_batched_tokens',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '8192',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--enable_prefix_caching',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--enable_sleep_mode',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--disable_custom_all_reduce',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--gpu_memory_utilization',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '0.5',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--disable_log_stats',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--tensor_parallel_size',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '1',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--seed',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '0',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '--override_generation_config',\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '{\"temperature\": 1.0, \"top_k\": -1, \"top_p\": 1, \"repetition_penalty\": 1.0, '\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202)  '\"max_new_tokens\": 4096}']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO:2025-12-13 07:20:11,116:replica_rank=0, node_rank=0, nnodes=1, get worker zmq addresses: ['ipc:///tmp/verl_vllm_zmq_1201697_tiger.ipc']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:20:17 [config.py:1604] Using max model len 8192\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) WARNING 12-13 07:20:17 [arg_utils.py:1695] Detected VLLM_USE_V1=1 with Engine in background thread. Usage should be considered experimental. Please report any issues on Github.\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:20:17 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) WARNING 12-13 07:20:17 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: In a Ray actor and can only be spawned\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:20:23 [__init__.py:235] Automatically detected platform cuda.\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:20:28 [core.py:572] Waiting for init message from front-end.\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:20:28 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/home/tiger/Qwen/Qwen3-1.7B', speculative_config=None, tokenizer='/home/tiger/Qwen/Qwen3-1.7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/tiger/Qwen/Qwen3-1.7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) INFO 12-13 07:20:32 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) WARNING 12-13 07:20:32 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) INFO 12-13 07:20:32 [gpu_model_runner.py:1843] Starting to load model /home/tiger/Qwen/Qwen3-1.7B...\n",
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) INFO 12-13 07:20:32 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) INFO 12-13 07:20:32 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.25it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.25it/s]\n",
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) INFO 12-13 07:20:33 [default_loader.py:262] Loading weights took 0.71 seconds\n",
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) INFO 12-13 07:20:34 [gpu_model_runner.py:1892] Model loading took 3.2152 GiB and 0.856083 seconds\n",
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) INFO 12-13 07:20:42 [backends.py:530] Using cache directory: /home/tiger/.cache/vllm/torch_compile_cache/3f4beb645d/rank_0_0/backbone for vLLM's torch.compile\n",
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) INFO 12-13 07:20:42 [backends.py:541] Dynamo bytecode transform time: 7.65 s\n",
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) INFO 12-13 07:20:47 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) INFO 12-13 07:21:15 [backends.py:215] Compiling a graph for dynamic shape takes 33.24 s\n",
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) INFO 12-13 07:21:23 [monitor.py:34] torch.compile takes 40.88 s in total\n",
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) INFO 12-13 07:21:24 [gpu_worker.py:255] Available KV cache memory: 30.87 GiB\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:21:24 [kv_cache_utils.py:833] GPU KV cache size: 289,024 tokens\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:21:24 [kv_cache_utils.py:837] Maximum concurrency for 8,192 tokens per request: 35.28x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   6%|         | 4/67 [00:00<00:01, 35.77it/s]\n",
      "Capturing CUDA graph shapes:  12%|        | 8/67 [00:00<00:01, 36.59it/s]\n",
      "Capturing CUDA graph shapes:  18%|        | 12/67 [00:00<00:01, 36.43it/s]\n",
      "Capturing CUDA graph shapes:  24%|       | 16/67 [00:00<00:01, 36.67it/s]\n",
      "Capturing CUDA graph shapes:  30%|       | 20/67 [00:00<00:01, 37.13it/s]\n",
      "Capturing CUDA graph shapes:  36%|      | 24/67 [00:00<00:01, 37.41it/s]\n",
      "Capturing CUDA graph shapes:  42%|     | 28/67 [00:00<00:01, 37.18it/s]\n",
      "Capturing CUDA graph shapes:  48%|     | 32/67 [00:00<00:00, 37.04it/s]\n",
      "Capturing CUDA graph shapes:  54%|    | 36/67 [00:00<00:00, 35.87it/s]\n",
      "Capturing CUDA graph shapes:  60%|    | 40/67 [00:01<00:00, 35.64it/s]\n",
      "Capturing CUDA graph shapes:  66%|   | 44/67 [00:01<00:00, 35.65it/s]\n",
      "Capturing CUDA graph shapes:  72%|  | 48/67 [00:01<00:00, 34.58it/s]\n",
      "Capturing CUDA graph shapes:  78%|  | 52/67 [00:01<00:00, 34.68it/s]\n",
      "Capturing CUDA graph shapes:  84%| | 56/67 [00:01<00:00, 34.86it/s]\n",
      "Capturing CUDA graph shapes:  90%| | 60/67 [00:01<00:00, 34.54it/s]\n",
      "Capturing CUDA graph shapes:  96%|| 64/67 [00:01<00:00, 33.24it/s]\n",
      "Capturing CUDA graph shapes: 100%|| 67/67 [00:01<00:00, 35.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(vLLMAsyncRollout pid=1201697, ip=10.122.252.202) INFO 12-13 07:21:27 [gpu_model_runner.py:2485] Graph capturing finished in 2 secs, took 0.49 GiB\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:21:27 [core.py:193] init engine (profile, create kv cache, warmup model) took 53.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO:2025-12-13 07:21:27,926:Initializing a V1 LLM engine with config: model='/home/tiger/Qwen/Qwen3-1.7B', speculative_config=None, tokenizer='/home/tiger/Qwen/Qwen3-1.7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/tiger/Qwen/Qwen3-1.7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) WARNING 12-13 07:21:27 [config.py:1528] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:21:27 [serving_responses.py:89] Using default chat sampling params from model: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'max_tokens': 4096}\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:21:27 [serving_chat.py:122] Using default chat sampling params from model: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'max_tokens': 4096}\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:21:27 [serving_completion.py:77] Using default completion sampling params from model: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'max_tokens': 4096}\n",
      "Rollout server address: 10.122.252.202:47645\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download a small-ish instruct model (adjust if you already have one).\n",
    "model_path = os.path.expanduser(\"~/Qwen/Qwen3-1.7B\")\n",
    "snapshot_download(repo_id=\"Qwen/Qwen3-1.7B\", repo_type=\"model\", local_dir=model_path)\n",
    "\n",
    "with initialize_config_dir(config_dir=verl_config_dir):\n",
    "    config = compose(\n",
    "        config_name=\"ppo_trainer\",\n",
    "        overrides=[\n",
    "            # rollout engine\n",
    "            \"actor_rollout_ref.rollout.name=\" + rollout_name,\n",
    "            \"actor_rollout_ref.rollout.mode=async\",\n",
    "            \"actor_rollout_ref.rollout.tensor_model_parallel_size=1\",\n",
    "            \"actor_rollout_ref.rollout.data_parallel_size=1\",\n",
    "            \"actor_rollout_ref.rollout.pipeline_model_parallel_size=1\",\n",
    "            \"actor_rollout_ref.rollout.skip_tokenizer_init=False\",\n",
    "            \"actor_rollout_ref.rollout.prompt_length=4096\",\n",
    "            \"actor_rollout_ref.rollout.response_length=4096\",\n",
    "            # model\n",
    "            \"actor_rollout_ref.model.path=\" + model_path,\n",
    "            # agent loop: use our FoldAgent React agent\n",
    "            \"actor_rollout_ref.rollout.agent.default_agent_loop=react_agent\",\n",
    "            \"actor_rollout_ref.rollout.agent.num_workers=1\",\n",
    "            # trainer sizing\n",
    "            \"trainer.n_gpus_per_node=2\",\n",
    "            \"trainer.nnodes=1\",\n",
    "            \"trainer.logger=['console']\",\n",
    "            \"trainer.project_name=verl\",\n",
    "            \"trainer.experiment_name=\" + os.path.basename(model_path)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Make a safe copy of the trainer config to attach plugin without affecting rollout server instantiation\n",
    "trainer_config_with_plugin = OmegaConf.create(OmegaConf.to_container(config, resolve=False))\n",
    "OmegaConf.set_struct(trainer_config_with_plugin.actor_rollout_ref.rollout, False)\n",
    "\n",
    "# Inject FoldAgent plugin fields on the copied config\n",
    "trainer_config_with_plugin.actor_rollout_ref.rollout.plugin = OmegaConf.create({\n",
    "    \"workflow\": \"search\",\n",
    "    \"max_turn\": 32,\n",
    "    \"retry_cjk\": 0,\n",
    "    \"turn_max_new_tokens\": 1024,\n",
    "    \"max_session\": 3,\n",
    "    \"val_max_session\": 3,\n",
    "    \"session_timeout\": 3600,\n",
    "    \"enable_summary\": True,\n",
    "    \"branch_len\": 256,\n",
    "    \"process_reward\": \"flat,scope\",\n",
    "    \"max_traj\": 2,\n",
    "    \"must_finish\": False,\n",
    "    \"double_check\": False,\n",
    "    \"must_search\": True,\n",
    "    \"val_max_turn\": 32,\n",
    "    \"val_response_length\": 1024,\n",
    "})\n",
    "\n",
    "print(\"Plugin config:\", OmegaConf.to_container(trainer_config_with_plugin.actor_rollout_ref.rollout.plugin, resolve=True))\n",
    "\n",
    "# Start a standalone rollout server (same as in the tutorial)\n",
    "rollout_server_class = get_rollout_replica_class(config.actor_rollout_ref.rollout.name)\n",
    "rollout_server = rollout_server_class(\n",
    "    replica_rank=0,\n",
    "    config=config.actor_rollout_ref.rollout,\n",
    "    model_config=config.actor_rollout_ref.model,\n",
    "    gpus_per_node=config.trainer.n_gpus_per_node,\n",
    ")\n",
    "await rollout_server.init_standalone()\n",
    "print(\"Rollout server address:\", rollout_server.server_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Build one test sample and run the agent loop\n",
    "We create a `DataProto` with `ability=LocalSearch` and the fields expected by your env and agent logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AgentLoopWorker pid=1203418, ip=10.122.252.202) INFO 12-13 07:21:42 [__init__.py:235] Automatically detected platform cuda.\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:21:44 [async_llm.py:269] Added request d762184fdc284ef2ba40a5fd9d89d6b7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(AgentLoopWorker pid=1203418, ip=10.122.252.202) ERROR:2025-12-13 07:21:44,355:Error getting data from env: 'NoneType' object has no attribute 'is_train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:21:46 [async_llm.py:269] Added request 0497c552ecaa46d6bc1eb6aaf1d45f26.\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:21:46 [async_llm.py:269] Added request d98a120d33484b05ac47faa2ce122f48.\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:21:46 [async_llm.py:269] Added request 1daaa291709f406d962aa01d718e3745.\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:21:46 [async_llm.py:269] Added request f91a6fbcaf294a4b917d14547add3dd2.\n",
      "(vLLMHttpServer pid=1201918, ip=10.122.252.202) INFO 12-13 07:21:47 [async_llm.py:269] Added request ca288248d7c940bc9a3589a92efdafde.\n",
      "Reward score: tensor([[0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Num turns: 13\n",
      "Extra fields keys: ['__num_turns__', 'env_stats', 'raw_prompt', 'messages']\n",
      "Counter({'action': 6,\n",
      "         'open_page': 3,\n",
      "         'visit_pages': 3,\n",
      "         'search': 2,\n",
      "         'is_search': 1,\n",
      "         'is_open': 1,\n",
      "         'is_finish': 1,\n",
      "         'finish': 0,\n",
      "         'change_answer': 0})\n",
      "\n",
      "Conversation transcript:\n",
      "Num turns: 13\n",
      "- system: You are a meticulous and strategic research agent. Your primary function is to conduct comprehensive, multi-step research to deliver a thorough, accurate, and well-supported report in response to the user's query.\n",
      "\n",
      "Your operation is guided by these core principles:\n",
      "* **Rigor:** Execute every step of the research process with precision and attention to detail.\n",
      "* **Objectivity:** Synthesize information based on the evidence gathered, not on prior assumptions. Note and investigate conflicting information.\n",
      "* **Thoroughness:** Never settle for a surface-level answer. Always strive to uncover the underlying details, context, and data.\n",
      "* **Transparency:** Your reasoning process should be clear at every step, linking evidence from your research directly to your conclusions.\n",
      "\n",
      "Follow this structured protocol for to find the answer\n",
      "\n",
      "### Phase 1: Deconstruction & Strategy\n",
      "\n",
      "1.  **Deconstruct the Query:**\n",
      "    * Analyze the user's prompt to identify the core question(s).\n",
      "    * Isolate key entities, concepts, and the relationships between them.\n",
      "    * Explicitly list all constraints, conditions, and required data points (e.g., dates, quantities, specific names).\n",
      "2.  **Hypothesize & Brainstorm:**\n",
      "    * Based on your knowledge, brainstorm potential search vectors, keywords, synonyms, and related topics that could yield relevant information.\n",
      "    * Consider multiple angles of inquiry to approach the problem.\n",
      "3.  **Verification Checklist:**\n",
      "    * Create a **Verification Checklist** based on the query's constraints and required data points. This checklist will be your guide throughout the process and used for final verification.\n",
      "\n",
      "### Phase 2: Iterative Research & Discovery\n",
      "\n",
      "**Tool Usage:**\n",
      "* **Tools:**\n",
      "    * `search`: Use for broad discovery of sources and to get initial snippets.\n",
      "    * `open_page`: **Mandatory follow-up** for any promising `search` result. Snippets are insufficient; you must analyze the full context of the source document.\n",
      "* **Query Strategy:**\n",
      "    * Start with moderately broad queries to map the information landscape. Narrow your focus as you learn more.\n",
      "    * Do not repeat the exact same query. If a query fails, rephrase it or change your angle of attack.\n",
      "    * Execute a **minimum of 5 tool calls** for simple queries and up to **50 tool calls** for complex ones. Do not terminate prematurely.\n",
      "* **Post-Action Analysis:** After every tool call, briefly summarize the key findings from the result, extract relevant facts, and explicitly state how this new information affects your next step in the OODA loop.\n",
      "* **<IMPORTANT>Never simulate tool call output<IMPORTANT>**\n",
      "\n",
      "You will execute your research plan using an iterative OODA loop (Observe, Orient, Decide, Act).\n",
      "\n",
      "1.  **Observe:** Review all gathered information. Identify what is known and, more importantly, what knowledge gaps remain according to your research plan.\n",
      "2.  **Orient:** Analyze the situation. Is the current line of inquiry effective? Are there new, more promising avenues? Refine your understanding of the topic based on the search results so far.\n",
      "3.  **Decide:** Choose the single most effective next action. This could be a broader query to establish context, a highly specific query to find a key data point, or opening a promising URL.\n",
      "4.  **Act:** Execute the chosen action using the available tools. After the action, return to **Observe**.\n",
      "\n",
      "### Phase 3: Synthesis & Analysis\n",
      "\n",
      "* **Continuous Synthesis:** Throughout the research process, continuously integrate new information with existing knowledge. Build a coherent narrative and understanding of the topic.\n",
      "* **Triangulate Critical Data:** For any crucial fact, number, date, or claim, you must seek to verify it across at least two independent, reliable sources. Note any discrepancies.\n",
      "* **Handle Dead Ends:** If you are blocked, do not give up. Broaden your search scope, try alternative keywords, or research related contextual information to uncover new leads. Assume a discoverable answer exists and exhaust all reasonable avenues.\n",
      "* **Maintain a \"Fact Sheet\":** Internally, keep a running list of key facts, figures, dates, and their supporting sources. This will be crucial for the final report.\n",
      "\n",
      "### Phase 4: Verification & Final Report Formulation\n",
      "\n",
      "1.  **Systematic Verification:** Before writing the final answer, halt your research and review your **Verification Checklist** created in Phase 1. For each item on the checklist, confirm you have sufficient, well-supported evidence from the documents you have opened.\n",
      "2.  **Mandatory Re-research:** If any checklist item is unconfirmed or the evidence is weak, it is **mandatory** to return to Phase 2 to conduct further targeted research. Do not formulate an answer based on incomplete information.\n",
      "3.  **Never give up**, no matter how complex the query, you will not give up until you find the corresponding information.\n",
      "4.  **Construct the Final Report:**\n",
      "    * Once all checklist items are confidently verified, synthesize all gathered facts into a comprehensive and well-structured answer.\n",
      "    * Directly answer the user's original query.\n",
      "    * Ensure all claims, numbers, and key pieces of information in your report are clearly supported by the research you conducted.\n",
      "\n",
      "Execute this entire protocol to provide a definitive and trustworthy answer to the user.\n",
      "\n",
      "\n",
      "\n",
      "You have access to the following functions:\n",
      "\n",
      "---- BEGIN FUNCTION #1: search ----\n",
      "Description: Performs a web search: supply a string 'query' and optional 'topk'. The tool retrieves the top 'topk' results (default 10) for the query, returning their docid, url, and document content (may be truncated based on token limits).\n",
      "Parameters:\n",
      "  (1) query (string, required): The query string for the search.\n",
      "  (2) topk (integer, optional): Return the top k pages.\n",
      "---- END FUNCTION #1 ----\n",
      "\n",
      "---- BEGIN FUNCTION #2: open_page ----\n",
      "Description: Open a page by docid or URL and return the complete content. Provide either 'docid' or 'url'; if both are provided, prefer 'docid'. The docid or URL must come from prior search tool results.\n",
      "Parameters:\n",
      "  (1) docid (string, optional): Document ID from search results to resolve and fetch.\n",
      "  (2) url (string, optional): Absolute URL from search results to fetch.\n",
      "---- END FUNCTION #2 ----\n",
      "\n",
      "---- BEGIN FUNCTION #3: finish ----\n",
      "Description: Return the final result when you have a definitive answer or cannot progress further. Provide a concise answer plus a brief, evidence-grounded explanation.\n",
      "Parameters:\n",
      "  (1) answer (string, required): A succinct, final answer.\n",
      "  (2) explanation (string, required): A brief explanation for your final answer. For this section only, cite evidence documents inline by placing their docids in square brackets at the end of sentences (e.g., [20]). Do not include citations anywhere else.\n",
      "  (3) confidence (string, optional): Confidence: your confidence score between 0% and 100% for your answer\n",
      "---- END FUNCTION #3 ----\n",
      "\n",
      "\n",
      "If you choose to call a function ONLY reply in the following format with NO suffix:\n",
      "\n",
      "<function=example_function_name>\n",
      "<parameter=example_parameter_1>value_1</parameter>\n",
      "<parameter=example_parameter_2>\n",
      "This is the value for the second parameter\n",
      "that can span\n",
      "multiple lines\n",
      "</parameter>\n",
      "</function>\n",
      "\n",
      "<IMPORTANT>\n",
      "Reminder:\n",
      "- Function calls MUST follow the specified format, start with <function= and end with </function>\n",
      "- Required parameters MUST be specified\n",
      "- You may provide optional reasoning for your function call in natural language BEFORE the function call, but NOT after.\n",
      "- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls\n",
      "</IMPORTANT>\n",
      "\n",
      "- user: You are a deep research agent. You need to answer the given question by interacting with a search engine, using the search and open tools provided. Please perform reasoning and use the tools step by step, in an interleaved manner. You may use the search and open tools multiple times.\n",
      "\n",
      "Question: Who is the CEO of Tesla?\n",
      "\n",
      "* You can search one queries:\n",
      "<function=search>\n",
      "<parameter=query>Query</parameter>\n",
      "<parameter=topk>10</parameter>\n",
      "</function>\n",
      "\n",
      "* Or you can search multiple queries in one turn by including multiple <function=search> actions, e.g.\n",
      "<function=search>\n",
      "<parameter=query>Query1</parameter>\n",
      "<parameter=topk>5</parameter>\n",
      "</function>\n",
      "<function=search>\n",
      "<parameter=query>Query2</parameter>\n",
      "<parameter=topk>5</parameter>\n",
      "</function>\n",
      "\n",
      "* Use open_page to fetch a web page:\n",
      "<function=open_page>\n",
      "<parameter=docid>docid</parameter>\n",
      "</function>\n",
      "or\n",
      "<function=open_page>\n",
      "<parameter=url>url</parameter>\n",
      "</function>\n",
      "\n",
      "Your response should contain:\n",
      "Explanation: {your explanation for your final answer. For this explanation section only, you should cite your evidence documents inline by enclosing their docids in square brackets [] at the end of sentences. For example, [20].}\n",
      "Exact Answer: {your succinct, final answer}\n",
      "Confidence: {your confidence score between 0% and 100% for your answer}\n",
      "Use finish tool to submit your answer.\n",
      "\n",
      "<IMPORTANT>\n",
      "- Always call a tool to get search results; never simulate a tool call.\n",
      "- You may provide optional reasoning for your function call in natural language BEFORE the function call, but NOT after.\n",
      "</IMPORTANT>\n",
      "\n",
      "- assistant: <think>\n",
      "Okay, I need to find out who the CEO of Tesla is. Let me start by searching for the latest information. I'll use the search function with the query \"Tesla CEO\" and topk=10 to get the top results. \n",
      "\n",
      "First, I'll perform the search to see what sources come up. The results might include articles, official websites, or news reports. I'll look for recent entries since Tesla's CEO can change, and I want the most up-to-date information.\n",
      "\n",
      "After getting the search results, I'll check the URLs to see which ones are trustworthy. If a result is from Tesla's official site, that's a good source. If it's a news article, I'll need to verify the information. If there's a conflicting report, I'll need to cross-reference it with another source.\n",
      "\n",
      "If I find the current CEO's name, I'll make sure it's from a reputable source. If there's a discrepancy, I'll need to open the relevant pages to check the details. For example, if the search returns a recent article stating Elon Musk is the CEO, but another source says Tony Stark, I'll need to verify which one is correct.\n",
      "\n",
      "Once I have the confirmed information, I'll compile the answer with the confidence level based on the evidence from the sources. If the information is from multiple reliable sources, the confidence will be high. If only one source is cited, the confidence might be lower.\n",
      "</think>\n",
      "\n",
      "<function=search>\n",
      "<parameter=query>Tesla CEO</parameter>\n",
      "<parameter=topk>10</parameter>\n",
      "</function>\n",
      "- user: [Search Results for \"Tesla CEO\"]\n",
      "\n",
      "--- #1: wiki:elon---\n",
      "docid: wiki:elon\n",
      "url: https://en.wikipedia.org/wiki/Elon_Musk\n",
      "content: Elon Musk is the CEO of Tesla, Inc. He also leads SpaceX, xAI, and other ventures.\n",
      "\n",
      "--- #2: tesla:leadership---\n",
      "docid: tesla:leadership\n",
      "url: https://www.tesla.com/leadership\n",
      "content: Tesla's CEO is Elon Musk. The leadership page lists executive roles and bios.\n",
      "\n",
      "--- #3: news:tesla---\n",
      "docid: news:tesla\n",
      "url: https://example.com/news/tesla\n",
      "content: Breaking: Tesla maintains its leadership under CEO Elon Musk, focusing on EV innovation.\n",
      "\n",
      "\n",
      "\n",
      "* Please reflect on the information we have obtained, and keep searching for additional information if we still can not answer the question. Do not give the answer if the information is still not enough.\n",
      "- assistant: <think>\n",
      "<function=search>\n",
      "<parameter=query>Tesla CEO</parameter>\n",
      "<parameter=topk>10</parameter>\n",
      "</function>\n",
      "- user: [Search Results for \"Tesla CEO\"]\n",
      "\n",
      "--- #1: wiki:elon---\n",
      "docid: wiki:elon\n",
      "url: https://en.wikipedia.org/wiki/Elon_Musk\n",
      "content: (This page was already seen in a previous search. Here, a shorter snippet is shown. If you find this page relevant, please use the open_page tool to inspect the full content) Elon Musk is the CEO of Tesla, Inc. He also leads SpaceX, xAI, and other ventures.\n",
      "\n",
      "--- #2: tesla:leadership---\n",
      "docid: tesla:leadership\n",
      "url: https://www.tesla.com/leadership\n",
      "content: (This page was already seen in a previous search. Here, a shorter snippet is shown. If you find this page relevant, please use the open_page tool to inspect the full content) Tesla's CEO is Elon Musk. The leadership page lists executive roles and bios.\n",
      "\n",
      "--- #3: news:tesla---\n",
      "docid: news:tesla\n",
      "url: https://example.com/news/tesla\n",
      "content: (This page was already seen in a previous search. Here, a shorter snippet is shown. If you find this page relevant, please use the open_page tool to inspect the full content) Breaking: Tesla maintains its leadership under CEO Elon Musk, focusing on EV innovation.\n",
      "\n",
      "\n",
      "\n",
      "* Please reflect on the information we have obtained, and keep searching for additional information if we still can not answer the question. Do not give the answer if the information is still not enough.\n",
      "- assistant: <think>\n",
      "<function=open_page>\n",
      "<parameter=docid>wiki:elon</parameter>\n",
      "</function>\n",
      "- user: [Opened Page Content]\n",
      "docid: wiki:elon\n",
      "url: https://en.wikipedia.org/wiki/Elon_Musk\n",
      "content: Elon Musk is the CEO of Tesla, Inc. He also leads SpaceX, xAI, and other ventures.\n",
      "\n",
      "\n",
      "\n",
      "* Please reflect on the information we have obtained, and keep searching for additional information if we still can not answer the question. Do not give the answer if the information is still not enough.\n",
      "- assistant: <think>\n",
      "<function=open_page>\n",
      "<parameter=docid>tesla:leadership</parameter>\n",
      "</function>\n",
      "- user: [Opened Page Content]\n",
      "docid: tesla:leadership\n",
      "url: https://www.tesla.com/leadership\n",
      "content: Tesla's CEO is Elon Musk. The leadership page lists executive roles and bios.\n",
      "\n",
      "\n",
      "\n",
      "* Please reflect on the information we have obtained, and keep searching for additional information if we still can not answer the question. Do not give the answer if the information is still not enough.\n",
      "- assistant: <think>\n",
      "<function=open_page>\n",
      "<parameter=docid>news:tesla</parameter>\n",
      "</function>\n",
      "- user: [Opened Page Content]\n",
      "docid: news:tesla\n",
      "url: https://example.com/news/tesla\n",
      "content: Breaking: Tesla maintains its leadership under CEO Elon Musk, focusing on EV innovation.\n",
      "\n",
      "\n",
      "\n",
      "* Please reflect on the information we have obtained, and keep searching for additional information if we still can not answer the question. Do not give the answer if the information is still not enough.\n",
      "- assistant: <think>\n",
      "<function=finish>\n",
      "<parameter=answer>Elon Musk</parameter>\n",
      "<parameter=explanation>Elon Musk is confirmed as the CEO of Tesla based on multiple reputable sources, including Wikipedia and Tesla's official leadership page. The information is consistent across different news outlets.</parameter>\n",
      "<parameter=confidence>100%</parameter>\n",
      "</function>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AgentLoopWorker pid=1203418, ip=10.122.252.202) [Judged] score=1\n",
      "(AgentLoopWorker pid=1203418, ip=10.122.252.202) Label: Elon Musk\n",
      "(AgentLoopWorker pid=1203418, ip=10.122.252.202) Model: Elon Musk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(AgentLoopWorker pid=1203418, ip=10.122.252.202) You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Minimal raw prompt (env will replace via its create_chat flow, but we include for completeness)\n",
    "raw_prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful agent that uses ReAct-style tool calls.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who is the CEO of Tesla? Provide citations.\"}\n",
    "]\n",
    "\n",
    "extra_info = {\n",
    "    \"query\": \"Who is the CEO of Tesla?\",\n",
    "    \"answer\": \"Elon Musk\",  # label for reward checking\n",
    "    \"prompt\": raw_prompt,\n",
    "    \"workflow\": \"search\"\n",
    "}\n",
    "\n",
    "uid = \"test-0001\"\n",
    "reward_model = \"default\"\n",
    "\n",
    "batch = DataProto.from_dict(\n",
    "    tensors={},\n",
    "    non_tensors={\n",
    "        \"raw_prompt\": np.array([raw_prompt], dtype=object),\n",
    "        \"extra_info\": np.array([extra_info], dtype=object),\n",
    "        \"uid\": np.array([uid], dtype=object),\n",
    "        \"reward_model\": np.array([reward_model], dtype=object),\n",
    "        \"ability\": np.array([\"LocalSearch\"], dtype=object),\n",
    "        \"agent_name\": np.array([\"react_agent\"], dtype=object),\n",
    "        \"index\": np.array([0], dtype=object)\n",
    "    },\n",
    "    meta_info={\"validate\": False, \"global_steps\": 0}\n",
    ")\n",
    "\n",
    "\n",
    "# Use AgentLoopWorker directly to avoid nested asyncio.run issues in notebooks\n",
    "from verl.experimental.agent_loop import AgentLoopWorker\n",
    "\n",
    "alm_worker = AgentLoopWorker.options(\n",
    "        name=\"notebook_agent_loop_worker\",\n",
    "        runtime_env={\"env_vars\": {\"LOCAL_SEARCH_URL\": f\"http://{search_address}\"}},\n",
    ").remote(\n",
    "        trainer_config_with_plugin,\n",
    "        [rollout_server.server_handle],  # reuse the already-started standalone server\n",
    "        None,\n",
    ")\n",
    "output = ray.get(alm_worker.generate_sequences.remote(batch))\n",
    "\n",
    "print(\"Reward score:\", output.batch.get(\"rm_scores\", None))\n",
    "print(\"Num turns:\", output.non_tensor_batch[\"__num_turns__\"][0])\n",
    "print(\"Extra fields keys:\", list(output.non_tensor_batch.keys()))\n",
    "pprint(output.non_tensor_batch.get(\"env_stats\", [None])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation transcript:\n",
      "Num turns: 13\n",
      "====================================================================================================\n",
      "- system: You are a meticulous and strategic research agent. Your primary function is to conduct comprehensive, multi-step research to deliver a thorough, accurate, and well-supported report in response to the user's query.\n",
      "\n",
      "Your operation is guided by these core principles:\n",
      "* **Rigor:** Execute every step of the research process with precision and attention to detail.\n",
      "* **Objectivity:** Synthesize information based on the evidence gathered, not on prior assumptions. Note and investigate conflicting information.\n",
      "* **Thoroughness:** Never settle for a surface-level answer. Always strive to uncover the underlying details, context, and data.\n",
      "* **Transparency:** Your reasoning process should be clear at every step, linking evidence from your research directly to your conclusions.\n",
      "\n",
      "Follow this structured protocol for to find the answer\n",
      "\n",
      "### Phase 1: Deconstruction & Strategy\n",
      "\n",
      "1.  **Deconstruct the Query:**\n",
      "    * Analyze the user's prompt to identify the core question(s).\n",
      "    * Isolate key entities, concepts, and the relationships between them.\n",
      "    * Explicitly list all constraints, conditions, and required data points (e.g., dates, quantities, specific names).\n",
      "2.  **Hypothesize & Brainstorm:**\n",
      "    * Based on your knowledge, brainstorm potential search vectors, keywords, synonyms, and related topics that could yield relevant information.\n",
      "    * Consider multiple angles of inquiry to approach the problem.\n",
      "3.  **Verification Checklist:**\n",
      "    * Create a **Verification Checklist** based on the query's constraints and required data points. This checklist will be your guide throughout the process and used for final verification.\n",
      "\n",
      "### Phase 2: Iterative Research & Discovery\n",
      "\n",
      "**Tool Usage:**\n",
      "* **Tools:**\n",
      "    * `search`: Use for broad discovery of sources and to get initial snippets.\n",
      "    * `open_page`: **Mandatory follow-up** for any promising `search` result. Snippets are insufficient; you must analyze the full context of the source document.\n",
      "* **Query Strategy:**\n",
      "    * Start with moderately broad queries to map the information landscape. Narrow your focus as you learn more.\n",
      "    * Do not repeat the exact same query. If a query fails, rephrase it or change your angle of attack.\n",
      "    * Execute a **minimum of 5 tool calls** for simple queries and up to **50 tool calls** for complex ones. Do not terminate prematurely.\n",
      "* **Post-Action Analysis:** After every tool call, briefly summarize the key findings from the result, extract relevant facts, and explicitly state how this new information affects your next step in the OODA loop.\n",
      "* **<IMPORTANT>Never simulate tool call output<IMPORTANT>**\n",
      "\n",
      "You will execute your research plan using an iterative OODA loop (Observe, Orient, Decide, Act).\n",
      "\n",
      "1.  **Observe:** Review all gathered information. Identify what is known and, more importantly, what knowledge gaps remain according to your research plan.\n",
      "2.  **Orient:** Analyze the situation. Is the current line of inquiry effective? Are there new, more promising avenues? Refine your understanding of the topic based on the search results so far.\n",
      "3.  **Decide:** Choose the single most effective next action. This could be a broader query to establish context, a highly specific query to find a key data point, or opening a promising URL.\n",
      "4.  **Act:** Execute the chosen action using the available tools. After the action, return to **Observe**.\n",
      "\n",
      "### Phase 3: Synthesis & Analysis\n",
      "\n",
      "* **Continuous Synthesis:** Throughout the research process, continuously integrate new information with existing knowledge. Build a coherent narrative and understanding of the topic.\n",
      "* **Triangulate Critical Data:** For any crucial fact, number, date, or claim, you must seek to verify it across at least two independent, reliable sources. Note any discrepancies.\n",
      "* **Handle Dead Ends:** If you are blocked, do not give up. Broaden your search scope, try alternative keywords, or research related contextual information to uncover new leads. Assume a discoverable answer exists and exhaust all reasonable avenues.\n",
      "* **Maintain a \"Fact Sheet\":** Internally, keep a running list of key facts, figures, dates, and their supporting sources. This will be crucial for the final report.\n",
      "\n",
      "### Phase 4: Verification & Final Report Formulation\n",
      "\n",
      "1.  **Systematic Verification:** Before writing the final answer, halt your research and review your **Verification Checklist** created in Phase 1. For each item on the checklist, confirm you have sufficient, well-supported evidence from the documents you have opened.\n",
      "2.  **Mandatory Re-research:** If any checklist item is unconfirmed or the evidence is weak, it is **mandatory** to return to Phase 2 to conduct further targeted research. Do not formulate an answer based on incomplete information.\n",
      "3.  **Never give up**, no matter how complex the query, you will not give up until you find the corresponding information.\n",
      "4.  **Construct the Final Report:**\n",
      "    * Once all checklist items are confidently verified, synthesize all gathered facts into a comprehensive and well-structured answer.\n",
      "    * Directly answer the user's original query.\n",
      "    * Ensure all claims, numbers, and key pieces of information in your report are clearly supported by the research you conducted.\n",
      "\n",
      "Execute this entire protocol to provide a definitive and trustworthy answer to the user.\n",
      "\n",
      "\n",
      "\n",
      "You have access to the following functions:\n",
      "\n",
      "---- BEGIN FUNCTION #1: search ----\n",
      "Description: Performs a web search: supply a string 'query' and optional 'topk'. The tool retrieves the top 'topk' results (default 10) for the query, returning their docid, url, and document content (may be truncated based on token limits).\n",
      "Parameters:\n",
      "  (1) query (string, required): The query string for the search.\n",
      "  (2) topk (integer, optional): Return the top k pages.\n",
      "---- END FUNCTION #1 ----\n",
      "\n",
      "---- BEGIN FUNCTION #2: open_page ----\n",
      "Description: Open a page by docid or URL and return the complete content. Provide either 'docid' or 'url'; if both are provided, prefer 'docid'. The docid or URL must come from prior search tool results.\n",
      "Parameters:\n",
      "  (1) docid (string, optional): Document ID from search results to resolve and fetch.\n",
      "  (2) url (string, optional): Absolute URL from search results to fetch.\n",
      "---- END FUNCTION #2 ----\n",
      "\n",
      "---- BEGIN FUNCTION #3: finish ----\n",
      "Description: Return the final result when you have a definitive answer or cannot progress further. Provide a concise answer plus a brief, evidence-grounded explanation.\n",
      "Parameters:\n",
      "  (1) answer (string, required): A succinct, final answer.\n",
      "  (2) explanation (string, required): A brief explanation for your final answer. For this section only, cite evidence documents inline by placing their docids in square brackets at the end of sentences (e.g., [20]). Do not include citations anywhere else.\n",
      "  (3) confidence (string, optional): Confidence: your confidence score between 0% and 100% for your answer\n",
      "---- END FUNCTION #3 ----\n",
      "\n",
      "\n",
      "If you choose to call a function ONLY reply in the following format with NO suffix:\n",
      "\n",
      "<function=example_function_name>\n",
      "<parameter=example_parameter_1>value_1</parameter>\n",
      "<parameter=example_parameter_2>\n",
      "This is the value for the second parameter\n",
      "that can span\n",
      "multiple lines\n",
      "</parameter>\n",
      "</function>\n",
      "\n",
      "<IMPORTANT>\n",
      "Reminder:\n",
      "- Function calls MUST follow the specified format, start with <function= and end with </function>\n",
      "- Required parameters MUST be specified\n",
      "- You may provide optional reasoning for your function call in natural language BEFORE the function call, but NOT after.\n",
      "- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls\n",
      "</IMPORTANT>\n",
      "\n",
      "====================================================================================================\n",
      "- user: You are a deep research agent. You need to answer the given question by interacting with a search engine, using the search and open tools provided. Please perform reasoning and use the tools step by step, in an interleaved manner. You may use the search and open tools multiple times.\n",
      "\n",
      "Question: Who is the CEO of Tesla?\n",
      "\n",
      "* You can search one queries:\n",
      "<function=search>\n",
      "<parameter=query>Query</parameter>\n",
      "<parameter=topk>10</parameter>\n",
      "</function>\n",
      "\n",
      "* Or you can search multiple queries in one turn by including multiple <function=search> actions, e.g.\n",
      "<function=search>\n",
      "<parameter=query>Query1</parameter>\n",
      "<parameter=topk>5</parameter>\n",
      "</function>\n",
      "<function=search>\n",
      "<parameter=query>Query2</parameter>\n",
      "<parameter=topk>5</parameter>\n",
      "</function>\n",
      "\n",
      "* Use open_page to fetch a web page:\n",
      "<function=open_page>\n",
      "<parameter=docid>docid</parameter>\n",
      "</function>\n",
      "or\n",
      "<function=open_page>\n",
      "<parameter=url>url</parameter>\n",
      "</function>\n",
      "\n",
      "Your response should contain:\n",
      "Explanation: {your explanation for your final answer. For this explanation section only, you should cite your evidence documents inline by enclosing their docids in square brackets [] at the end of sentences. For example, [20].}\n",
      "Exact Answer: {your succinct, final answer}\n",
      "Confidence: {your confidence score between 0% and 100% for your answer}\n",
      "Use finish tool to submit your answer.\n",
      "\n",
      "<IMPORTANT>\n",
      "- Always call a tool to get search results; never simulate a tool call.\n",
      "- You may provide optional reasoning for your function call in natural language BEFORE the function call, but NOT after.\n",
      "</IMPORTANT>\n",
      "\n",
      "====================================================================================================\n",
      "- assistant: <think>\n",
      "Okay, I need to find out who the CEO of Tesla is. Let me start by searching for the latest information. I'll use the search function with the query \"Tesla CEO\" and topk=10 to get the top results. \n",
      "\n",
      "First, I'll perform the search to see what sources come up. The results might include articles, official websites, or news reports. I'll look for recent entries since Tesla's CEO can change, and I want the most up-to-date information.\n",
      "\n",
      "After getting the search results, I'll check the URLs to see which ones are trustworthy. If a result is from Tesla's official site, that's a good source. If it's a news article, I'll need to verify the information. If there's a conflicting report, I'll need to cross-reference it with another source.\n",
      "\n",
      "If I find the current CEO's name, I'll make sure it's from a reputable source. If there's a discrepancy, I'll need to open the relevant pages to check the details. For example, if the search returns a recent article stating Elon Musk is the CEO, but another source says Tony Stark, I'll need to verify which one is correct.\n",
      "\n",
      "Once I have the confirmed information, I'll compile the answer with the confidence level based on the evidence from the sources. If the information is from multiple reliable sources, the confidence will be high. If only one source is cited, the confidence might be lower.\n",
      "</think>\n",
      "\n",
      "<function=search>\n",
      "<parameter=query>Tesla CEO</parameter>\n",
      "<parameter=topk>10</parameter>\n",
      "</function>\n",
      "====================================================================================================\n",
      "- user: [Search Results for \"Tesla CEO\"]\n",
      "\n",
      "--- #1: wiki:elon---\n",
      "docid: wiki:elon\n",
      "url: https://en.wikipedia.org/wiki/Elon_Musk\n",
      "content: Elon Musk is the CEO of Tesla, Inc. He also leads SpaceX, xAI, and other ventures.\n",
      "\n",
      "--- #2: tesla:leadership---\n",
      "docid: tesla:leadership\n",
      "url: https://www.tesla.com/leadership\n",
      "content: Tesla's CEO is Elon Musk. The leadership page lists executive roles and bios.\n",
      "\n",
      "--- #3: news:tesla---\n",
      "docid: news:tesla\n",
      "url: https://example.com/news/tesla\n",
      "content: Breaking: Tesla maintains its leadership under CEO Elon Musk, focusing on EV innovation.\n",
      "\n",
      "\n",
      "\n",
      "* Please reflect on the information we have obtained, and keep searching for additional information if we still can not answer the question. Do not give the answer if the information is still not enough.\n",
      "====================================================================================================\n",
      "- assistant: <think>\n",
      "<function=search>\n",
      "<parameter=query>Tesla CEO</parameter>\n",
      "<parameter=topk>10</parameter>\n",
      "</function>\n",
      "====================================================================================================\n",
      "- user: [Search Results for \"Tesla CEO\"]\n",
      "\n",
      "--- #1: wiki:elon---\n",
      "docid: wiki:elon\n",
      "url: https://en.wikipedia.org/wiki/Elon_Musk\n",
      "content: (This page was already seen in a previous search. Here, a shorter snippet is shown. If you find this page relevant, please use the open_page tool to inspect the full content) Elon Musk is the CEO of Tesla, Inc. He also leads SpaceX, xAI, and other ventures.\n",
      "\n",
      "--- #2: tesla:leadership---\n",
      "docid: tesla:leadership\n",
      "url: https://www.tesla.com/leadership\n",
      "content: (This page was already seen in a previous search. Here, a shorter snippet is shown. If you find this page relevant, please use the open_page tool to inspect the full content) Tesla's CEO is Elon Musk. The leadership page lists executive roles and bios.\n",
      "\n",
      "--- #3: news:tesla---\n",
      "docid: news:tesla\n",
      "url: https://example.com/news/tesla\n",
      "content: (This page was already seen in a previous search. Here, a shorter snippet is shown. If you find this page relevant, please use the open_page tool to inspect the full content) Breaking: Tesla maintains its leadership under CEO Elon Musk, focusing on EV innovation.\n",
      "\n",
      "\n",
      "\n",
      "* Please reflect on the information we have obtained, and keep searching for additional information if we still can not answer the question. Do not give the answer if the information is still not enough.\n",
      "====================================================================================================\n",
      "- assistant: <think>\n",
      "<function=open_page>\n",
      "<parameter=docid>wiki:elon</parameter>\n",
      "</function>\n",
      "====================================================================================================\n",
      "- user: [Opened Page Content]\n",
      "docid: wiki:elon\n",
      "url: https://en.wikipedia.org/wiki/Elon_Musk\n",
      "content: Elon Musk is the CEO of Tesla, Inc. He also leads SpaceX, xAI, and other ventures.\n",
      "\n",
      "\n",
      "\n",
      "* Please reflect on the information we have obtained, and keep searching for additional information if we still can not answer the question. Do not give the answer if the information is still not enough.\n",
      "====================================================================================================\n",
      "- assistant: <think>\n",
      "<function=open_page>\n",
      "<parameter=docid>tesla:leadership</parameter>\n",
      "</function>\n",
      "====================================================================================================\n",
      "- user: [Opened Page Content]\n",
      "docid: tesla:leadership\n",
      "url: https://www.tesla.com/leadership\n",
      "content: Tesla's CEO is Elon Musk. The leadership page lists executive roles and bios.\n",
      "\n",
      "\n",
      "\n",
      "* Please reflect on the information we have obtained, and keep searching for additional information if we still can not answer the question. Do not give the answer if the information is still not enough.\n",
      "====================================================================================================\n",
      "- assistant: <think>\n",
      "<function=open_page>\n",
      "<parameter=docid>news:tesla</parameter>\n",
      "</function>\n",
      "====================================================================================================\n",
      "- user: [Opened Page Content]\n",
      "docid: news:tesla\n",
      "url: https://example.com/news/tesla\n",
      "content: Breaking: Tesla maintains its leadership under CEO Elon Musk, focusing on EV innovation.\n",
      "\n",
      "\n",
      "\n",
      "* Please reflect on the information we have obtained, and keep searching for additional information if we still can not answer the question. Do not give the answer if the information is still not enough.\n",
      "====================================================================================================\n",
      "- assistant: <think>\n",
      "<function=finish>\n",
      "<parameter=answer>Elon Musk</parameter>\n",
      "<parameter=explanation>Elon Musk is confirmed as the CEO of Tesla based on multiple reputable sources, including Wikipedia and Tesla's official leadership page. The information is consistent across different news outlets.</parameter>\n",
      "<parameter=confidence>100%</parameter>\n",
      "</function>\n"
     ]
    }
   ],
   "source": [
    "# If messages were captured by the agent loop, they are included under extra_fields\n",
    "messages = output.non_tensor_batch.get(\"messages\", [None])[0]\n",
    "if messages:\n",
    "    print(\"\\nConversation transcript:\")\n",
    "    print(f\"Num turns: {output.non_tensor_batch['__num_turns__'][0]}\")\n",
    "    for m in messages:\n",
    "        print(\"=\" * 100)\n",
    "        role = m.get(\"role\")\n",
    "        content = m.get(\"content\")\n",
    "        print(f\"- {role}: {content}\" if content else f\"- {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of output fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DataProto Summary ===\n",
      "\n",
      "Meta Info:\n",
      "{'metrics': [{'generate_sequences': 4.131077413447201,\n",
      "              'tool_calls': 0.02602100116200745}],\n",
      " 'reward_extra_keys': []}\n",
      "\n",
      "Batch (TensorDict):\n",
      "- prompts: shape=(1, 4096), dtype=torch.int64\n",
      "- responses: shape=(1, 4096), dtype=torch.int64\n",
      "- response_mask: shape=(1, 4096), dtype=torch.int64\n",
      "- input_ids: shape=(1, 8192), dtype=torch.int64\n",
      "- attention_mask: shape=(1, 8192), dtype=torch.int64\n",
      "- position_ids: shape=(1, 8192), dtype=torch.int64\n",
      "- rollout_log_probs: shape=(1, 4096), dtype=torch.float32\n",
      "- rm_scores: shape=(1, 4096), dtype=torch.float32\n",
      "\n",
      "Non-Tensor Batch:\n",
      "- __num_turns__: dtype=int32, shape=(1,)\n",
      "  head: [32]\n",
      "- env_stats: dtype=object, shape=(1,)\n",
      "  sample keys: ['finish', 'search', 'open_page', 'change_answer', 'is_search', 'is_open', 'is_finish', 'visit_pages', 'action']\n",
      "- raw_prompt: dtype=object, shape=(1,)\n",
      "  sample: [{'role': 'system', 'content': 'You are a helpful agent that uses ReAct-style tool calls.'}\n",
      " {'role': 'user', 'content': 'Who is the CEO of Tesla? Provide citations.'}]\n",
      "- messages: dtype=object, shape=(1,)\n",
      "  sample: [{'role': 'system', 'content': 'You are a meticulous and strategic research agent. Your primary function is to conduct comprehensive, multi-step research to deliver a thorough, accurate, and well-supported report in response to the user\\'s query.\\n\\nYour operation is guided by these core principles:\\n* **Rigor:** Execute every step of the research process with precision and attention to detail.\\n* **Objectivity:** Synthesize information based on the evidence gathered, not on prior assumptions. N...\n",
      "\n",
      "=== End Summary ===\n"
     ]
    }
   ],
   "source": [
    "# output = ray.get(alm_worker.generate_sequences.remote(batch))\n",
    "\n",
    "# Detailed summary of DataProto output\n",
    "\n",
    "def summarize_dataproto(dp):\n",
    "    print(\"=== DataProto Summary ===\")\n",
    "\n",
    "    # Meta info\n",
    "    print(\"\\nMeta Info:\")\n",
    "    pprint(dp.meta_info)\n",
    "\n",
    "    # Batch (TensorDict)\n",
    "    print(\"\\nBatch (TensorDict):\")\n",
    "    for k in list(dp.batch.keys()):\n",
    "        t = dp.batch[k]\n",
    "        try:\n",
    "            shape = tuple(t.shape)\n",
    "            dtype = t.dtype\n",
    "        except Exception:\n",
    "            shape = getattr(t, \"shape\", None)\n",
    "            dtype = getattr(t, \"dtype\", None)\n",
    "        print(f\"- {k}: shape={shape}, dtype={dtype}\")\n",
    "\n",
    "    # Non-tensor batch\n",
    "    print(\"\\nNon-Tensor Batch:\")\n",
    "    for k, v in dp.non_tensor_batch.items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            print(f\"- {k}: dtype={v.dtype}, shape={v.shape}\")\n",
    "            if v.dtype == object and v.size > 0:\n",
    "                sample = v[0]\n",
    "                if k == \"multi_modal_inputs\" and isinstance(sample, dict):\n",
    "                    print(\"  multi_modal_inputs sample tensors:\")\n",
    "                    for kk, val in sample.items():\n",
    "                        if hasattr(val, \"shape\"):\n",
    "                            print(f\"    {kk}: shape={tuple(val.shape)}, dtype={getattr(val, 'dtype', None)}\")\n",
    "                elif isinstance(sample, dict):\n",
    "                    print(f\"  sample keys: {list(sample.keys())}\")\n",
    "                else:\n",
    "                    s = str(sample)\n",
    "                    print(f\"  sample: {s[:500]}{'...' if len(s) > 500 else ''}\")\n",
    "            elif v.size > 0 and v.ndim == 1:\n",
    "                head = v[:min(5, v.shape[0])]\n",
    "                print(f\"  head: {head}\")\n",
    "        else:\n",
    "            print(f\"- {k}: type={type(v)}\")\n",
    "\n",
    "    # Messages preview (first sample)\n",
    "    # messages = dp.non_tensor_batch.get(\"messages\", None)\n",
    "    # if messages is not None and len(messages) > 0:\n",
    "    #     first = messages[0]\n",
    "    #     if isinstance(first, list):\n",
    "    #         print(\"\\nTranscript (first sample):\")\n",
    "    #         for m in first:\n",
    "    #             role = m.get(\"role\")\n",
    "    #             content = m.get(\"content\")\n",
    "    #             preview = content[:200] if isinstance(content, str) else content\n",
    "    #             print(f\"  - {role}: {preview}\")\n",
    "    #     else:\n",
    "    #         print(\"\\nMessages[0] type:\", type(first))\n",
    "\n",
    "    print(\"\\n=== End Summary ===\")\n",
    "\n",
    "summarize_dataproto(output)"
   ]
  }
 ],
 "metadata": {
  "fileId": "e4ade1f8-5663-4df8-8a5d-c89d34b7e71d",
  "filePath": "/opt/tiger/verl_context_folding/examples/tutorial/agent_loop_get_started/agent_loop_training_fold_agent.ipynb",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
